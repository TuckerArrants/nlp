{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "The Game is Afoot! XLM-RoBERTa + Translation Aug.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TuckerArrants/nlp/blob/master/XLM%20RoBERTa%20%2B%20Translation%20Aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTrZNeFEK-OP",
        "colab_type": "text"
      },
      "source": [
        "# It's Elementary, My Dear Watson\n",
        "\n",
        "**Natural Language Inference (NLI) is a specific type of NLP task where we must determine whether or not a hypothesis is true based on a premise. Specifically, given a pair of sentences, can we classify them into three different classes: 0 - entailment, 1 - contradiction, 2 - neutral?**\n",
        "\n",
        "**The current leading model in this field is RoBERTa, described by its creators as a 'robustly optimized BERT pretraining approach'. It changes some of the key hyperparameters of BERT and removes the next-sentece pretraining objective all together. The original paper can be found [here](https://arxiv.org/abs/1907.11692) and the source code [here](https://github.com/pytorch/fairseq/tree/master/examples/roberta)**\n",
        "\n",
        "**Now, we have 15 different languages in our dataset, so we cannot use the standard pre-trained RoBERTa model as it has only been trained on English sequences. Luckily, there is [XLM-RoBERTa](https://huggingface.co/transformers/model_doc/xlmroberta.html) (original paper can be found [here](https://arxiv.org/abs/1911.02116)) which has been trained on 2.5TB of filtered CommonCrawl data in 100 different languages. The implementation procedude is the same as RoBERTa's, so it is easy enough to deploy. Let's see how:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "IjZ816ZJK-OQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "22dc7708-850d-4d18-b3c0-c01e500546e9"
      },
      "source": [
        "#python basics\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import math, os, re, time, random\n",
        "import numpy as np, pandas as pd, seaborn as sns\n",
        "\n",
        "#deep learning basics\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "#nlp augmentation\n",
        "!pip install --quiet googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "#model evaluation\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "#for fast parallel processing\n",
        "from dask import bag, diagnostics\n",
        "\n",
        "#easy way to shuffle rows\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "#get current TensorFlow version fo\n",
        "print(\"Currently using Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Currently using Tensorflow version 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "grzgGBJ2K-OT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#choose devicve\n",
        "DEVICE = 'TPU'    #or GPU\n",
        "\n",
        "#choose number of epochs to train for\n",
        "EPOCHS = 12\n",
        "\n",
        "#choose seed\n",
        "SEED = 34\n",
        "\n",
        "#choose to see training progress or not\n",
        "VERBOSE = 2\n",
        "\n",
        "#choose maximum length of sequence inputs\n",
        "MAX_LEN = 80\n",
        "\n",
        "#choose learning rate for Adam optimizer\n",
        "LR_RATE = 1e-5\n",
        "\n",
        "#choose to add augmented text data or not\n",
        "GEN_AUG = False "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5un5UGwlyfpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "seed_everything(SEED)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUdfpF_2K-OV",
        "colab_type": "text"
      },
      "source": [
        "# EDA\n",
        "\n",
        "**A very brief data exploration; I will expand this section in future iterations of the notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oE_xjcJNK-OW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "0c1ff27d-9639-4b7b-929a-70403f627f93"
      },
      "source": [
        "#get CSV files\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f'Train shape: {train.shape}')\n",
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (12120, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5130fd2cb5</td>\n",
              "      <td>and these comments were considered in formulat...</td>\n",
              "      <td>The rules developed in the interim were put to...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5b72532a0b</td>\n",
              "      <td>These are issues that we wrestle with in pract...</td>\n",
              "      <td>Practice groups are not permitted to work on t...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3931fbe82a</td>\n",
              "      <td>Des petites choses comme celles-l√† font une di...</td>\n",
              "      <td>J'essayais d'accomplir quelque chose.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5622f0c60b</td>\n",
              "      <td>you know they can't really defend themselves l...</td>\n",
              "      <td>They can't defend themselves because of their ...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>86aaa48b45</td>\n",
              "      <td>‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡πà‡∏ô‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏Å‡πá‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÅ‡∏™‡∏î...</td>\n",
              "      <td>‡πÄ‡∏î‡πá‡∏Å‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£</td>\n",
              "      <td>th</td>\n",
              "      <td>Thai</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... label\n",
              "0  5130fd2cb5  ...     0\n",
              "1  5b72532a0b  ...     2\n",
              "2  3931fbe82a  ...     0\n",
              "3  5622f0c60b  ...     0\n",
              "4  86aaa48b45  ...     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TXaVG-qQK-OY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "3d16b396-aaac-4607-f895-4ec01262c6eb"
      },
      "source": [
        "print(f'Test shape: {test.shape}')\n",
        "test.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test shape: (5195, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c6d58c3f69</td>\n",
              "      <td>ÿ®⁄©ÿ≥ÿå ⁄©€åÿ≥€åÿå ÿ±ÿß€Å€åŸÑÿå €åÿ≥ÿπ€åÿß€Åÿå ⁄©€åŸÑ€åÿå ⁄©€åŸÑ€åÿå ÿßŸàÿ± ⁄©ŸàŸÑŸÖ...</td>\n",
              "      <td>⁄©€åÿ≥€å ⁄©€í ŸÑÿ¶€í ⁄©Ÿàÿ¶€å €åÿßÿØ⁄Øÿßÿ± ŸÜ€Å€å⁄∫ €ÅŸà⁄Øÿß, ⁄©ŸàŸÑŸÖ€åŸÜ €Åÿßÿ¶€å...</td>\n",
              "      <td>ur</td>\n",
              "      <td>Urdu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cefcc82292</td>\n",
              "      <td>Ÿáÿ∞ÿß ŸáŸà ŸÖÿß ÿ™ŸÖ ŸÜÿµÿ≠ŸÜÿß ÿ®Ÿá.</td>\n",
              "      <td>ÿπŸÜÿØŸÖÿß Ÿäÿ™ŸÖ ÿ•ÿÆÿ®ÿßÿ±ŸáŸÖ ÿ®ŸÖÿß Ÿäÿ¨ÿ® ÿπŸÑŸäŸáŸÖ ŸÅÿπŸÑŸá ÿå ŸÅÿ¥ŸÑÿ™ ÿßŸÑ...</td>\n",
              "      <td>ar</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e98005252c</td>\n",
              "      <td>et cela est en grande partie d√ª au fait que le...</td>\n",
              "      <td>Les m√®res se droguent.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58518c10ba</td>\n",
              "      <td>‰∏éÂüéÂ∏ÇÂèäÂÖ∂‰ªñÂÖ¨Ê∞ëÂèäÁ§æÂå∫ÁªÑÁªá‰ª£Ë°®Â∞±IMAÁöÑËâ∫ÊúØÂèëÂ±ïËøõË°åÂØπËØù&amp;amp</td>\n",
              "      <td>IMA‰∏éÂÖ∂‰ªñÁªÑÁªáÂêà‰ΩúÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÈÉΩ‰æùÈù†ÂÖ±‰∫´ËµÑÈáë„ÄÇ</td>\n",
              "      <td>zh</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c32b0d16df</td>\n",
              "      <td>–û–Ω–∞ –≤—Å–µ –µ—â–µ –±—ã–ª–∞ —Ç–∞–º.</td>\n",
              "      <td>–ú—ã –¥—É–º–∞–ª–∏, —á—Ç–æ –æ–Ω–∞ —É—à–ª–∞, –æ–¥–Ω–∞–∫–æ, –æ–Ω–∞ –æ—Å—Ç–∞–ª–∞—Å—å.</td>\n",
              "      <td>ru</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... language\n",
              "0  c6d58c3f69  ...     Urdu\n",
              "1  cefcc82292  ...   Arabic\n",
              "2  e98005252c  ...   French\n",
              "3  58518c10ba  ...  Chinese\n",
              "4  c32b0d16df  ...  Russian\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_vOtJH7tK-Oa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d35bb6ff-1c10-4718-c920-fb5535a1f1fc"
      },
      "source": [
        "#peek at a premise/hypothesis pair and their label\n",
        "print(f\"Premise: {train['premise'].values[0]}\")\n",
        "print(f\"Hypothesis: {train['hypothesis'].values[0]}\")\n",
        "print(f\"Label: {train['label'].values[0]}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Premise: and these comments were considered in formulating the interim rules.\n",
            "Hypothesis: The rules developed in the interim were put together with these comments in mind.\n",
            "Label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wKhxVI-TK-Oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "18be429d-415e-4138-a1bd-a67b6f67866b"
      },
      "source": [
        "#peek at a premise/hypothesis pair and their label\n",
        "print(f\"Premise: {train['premise'].values[1]}\")\n",
        "print(f\"Hypothesis: {train['hypothesis'].values[1]}\")\n",
        "print(f\"Label: {train['label'].values[1]}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Premise: These are issues that we wrestle with in practice groups of law firms, she said. \n",
            "Hypothesis: Practice groups are not permitted to work on these issues.\n",
            "Label: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "arEg6IHvK-Oe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "79752847-06ea-4c70-9bd2-a5ce31f5b917"
      },
      "source": [
        "#explore the distribution of classes and languages\n",
        "fig, ax = plt.subplots(3, figsize = (15, 10))\n",
        "\n",
        "#for maximum aesthetics\n",
        "palette = sns.cubehelix_palette(8, start=2, rot=0, dark=0, light=.95, reverse=True)\n",
        "\n",
        "graph1 = sns.countplot(train['language'], ax = ax[1], palette = palette)\n",
        "graph2 = sns.countplot(train['label'], ax = ax[0], palette = palette)\n",
        "graph3 = sns.countplot(test['language'], ax = ax[2], palette = palette)\n",
        "\n",
        "#set title\n",
        "graph1.set_title('Distribution of Classes')\n",
        "graph2.set_title('Distribution of Languages')\n",
        "graph3.set_title('Distribution of Languages - Test')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAH8B0Ddyfpo",
        "colab_type": "text"
      },
      "source": [
        "# Augmentation (Offline)\n",
        "\n",
        "**In computer vision problems, there is a virtual infinitude of techniques you can use to augment your images ranging from simple techniques like randomly flipping images to blending images together with CutMix or MixUp. In natural language processing, it is not as easy to come up with similar augmentation strategies because it is hard to determine which transformations will preserve the meaning of the original words:**\n",
        "\n",
        "![](https://amitness.com/images/semantic-invariance-nlp.png)\n",
        "*Image from [@amitness](https://www.kaggle.com/amitness) on his excellent post on NLP augmentation [here](https://amitness.com/2020/05/data-augmentation-for-nlp/)*   \n",
        "\n",
        "\n",
        "**I initially planned on using synonym swapping for augmentation, where we randomly replace words with their synonyms, but then I saw [this kernel](https://www.kaggle.com/jpmiller/augmenting-data-with-translations) which is based on [this discussion thread](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038) and realized we can do better: we can use translation to augment our data and do several things:**\n",
        "\n",
        "1. We can experiment and see if training our model on one language is better/worse than training on multiple languages\n",
        "2. We can create an English only dataset and a non-English dataset and merge the two to give our model more training samples\n",
        "3. We can randomly translate sentences to another language and then translate them back to the original like so:\n",
        "\n",
        "![](https://amitness.com/images/backtranslation-en-fr.png)\n",
        "\n",
        "*Image from [@amitness](https://www.kaggle.com/amitness) on his excellent post on NLP augmentation [here](https://amitness.com/2020/05/data-augmentation-for-nlp/)*\n",
        "\n",
        "**We can also apply this augmentation in two ways: offline augmentation or online augmentation. In the first, we augment before we feed to the model, adding to our dataset size. This is preferable for smaller datasets where we are not worried about training taking too long. When you can't afford an increase in size, you resort to online augmentation where augment the data every epoch. We will use offline augmentation in this commit (for more [see](https://www.kaggle.com/c/datasciencebowl/discussion/12597)). NLP augmentation is normally an offline processes, whereas computer vision augmentation is normally online.**\n",
        "\n",
        "**For now, we will only translate to languages currently present in our dataset, but translating to languages outside of our dataset might give us better performance. Please note that some of these language codes are slightly different within the `googletrans` Python API. See [here](https://py-googletrans.readthedocs.io/en/latest/) for more**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5XCxr_Puyfpo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "804919a9-1c1e-41ac-c2f0-acd877f02180"
      },
      "source": [
        "def translation_aug(sequence, PROB = 1):\n",
        "    languages = ['en', 'fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi',\n",
        "                 'sw', 'vi', 'es', 'el']\n",
        "    \n",
        "    #instantiate translator\n",
        "    translator = Translator()\n",
        "    \n",
        "    #store original language so we can convert back\n",
        "    org_lang = translator.detect(sequence).lang\n",
        "    \n",
        "    #randomly choose language to translate sequence to  \n",
        "    random_lang = np.random.choice([lang for lang in languages if lang is not org_lang])\n",
        "    \n",
        "    if org_lang in languages:\n",
        "        #translate to new language and back to original\n",
        "        translated = translator.translate(sequence, dest = random_lang).text\n",
        "        #translate back to original language\n",
        "        translated_back = translator.translate(translated, dest = org_lang).text\n",
        "    \n",
        "        #apply with certain probability\n",
        "        if np.random.uniform(0, 1) <= PROB:\n",
        "            output_sequence = translated_back\n",
        "        else:\n",
        "            output_sequence = sequence\n",
        "            \n",
        "    #if detected language not in our list of languages, do nothing\n",
        "    else:\n",
        "        output_sequence = sequence\n",
        "    \n",
        "    return output_sequence\n",
        "\n",
        "#check performance\n",
        "for i in range(5):\n",
        "    output = translation_aug('I genuinely have no idea what the output of this sequence of words will be')\n",
        "    print(output)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I really have no idea what this sequence of words will be like\n",
            "I really don't know what the output of this word sequence is\n",
            "I really have no idea what the output of this sequence of words would be\n",
            "I really don‚Äôt know what the results of this sequence of words will be\n",
            "I really have no idea what the result of this sequence of words will be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJI8Pniyfpq",
        "colab_type": "text"
      },
      "source": [
        "**We need to decide if we want to apply this translation augmentation to the premise/hypothesis at the same time, so each sequence is mapped to the same language and back, or if we want to map them to different languages and back. It's easier to do the latter so I will experiment with it in this commit**\n",
        "\n",
        "**This will also take a little while, so we can use `Dask Bag` to make it faster (more on Dask  [here](https://docs.dask.org/en/latest/bag.html)). Currently, it takes around 30 minutes to translate all the premise/hypothesis pairs from their original langauge, to a random language, and back again when `PROB = 1/2`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9VZ9wXnByfpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translation_aug_parallel(dataset):\n",
        "    prem_bag = bag.from_sequence(dataset['premise'].tolist()).map(translation_aug)\n",
        "    hyp_bag =  bag.from_sequence(dataset['hypothesis'].tolist()).map(translation_aug)\n",
        "    \n",
        "    with diagnostics.ProgressBar():\n",
        "        prems = prem_bag.compute()\n",
        "        hyps = hyp_bag.compute()\n",
        "\n",
        "    #pair premises and hypothesis\n",
        "    dataset[['premise', 'hypothesis']] = list(zip(prems, hyps))\n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc6GIMs2yfps",
        "colab_type": "text"
      },
      "source": [
        "**I have already created augmented datasets with the above translation method that can be found [here](https://www.kaggle.com/tuckerarrants/contradictorywatsontranslationaug) and [here](https://www.kaggle.com/tuckerarrants/contradictorywatsontwicetranslatedaug). Let's quickly compare the three separate datasets:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ivZakXGcyfps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "51d63909-ff15-45f6-9560-5c22719437b9"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5130fd2cb5</td>\n",
              "      <td>and these comments were considered in formulat...</td>\n",
              "      <td>The rules developed in the interim were put to...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5b72532a0b</td>\n",
              "      <td>These are issues that we wrestle with in pract...</td>\n",
              "      <td>Practice groups are not permitted to work on t...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3931fbe82a</td>\n",
              "      <td>Des petites choses comme celles-l√† font une di...</td>\n",
              "      <td>J'essayais d'accomplir quelque chose.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5622f0c60b</td>\n",
              "      <td>you know they can't really defend themselves l...</td>\n",
              "      <td>They can't defend themselves because of their ...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>86aaa48b45</td>\n",
              "      <td>‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡πà‡∏ô‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡∏Å‡πá‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÅ‡∏™‡∏î...</td>\n",
              "      <td>‡πÄ‡∏î‡πá‡∏Å‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£</td>\n",
              "      <td>th</td>\n",
              "      <td>Thai</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... label\n",
              "0  5130fd2cb5  ...     0\n",
              "1  5b72532a0b  ...     2\n",
              "2  3931fbe82a  ...     0\n",
              "3  5622f0c60b  ...     0\n",
              "4  86aaa48b45  ...     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2-cAIhBVyfpu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "a4ee72ee-c440-435e-931a-b98838adfb3d"
      },
      "source": [
        "#offline loading of augmented datasets\n",
        "train_aug = pd.read_csv('translation_aug_train.csv')\n",
        "train_aug.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5130fd2cb5</td>\n",
              "      <td>and these ideas were considered in the develop...</td>\n",
              "      <td>The rules developed in the interim were taken ...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5b72532a0b</td>\n",
              "      <td>These are the challenges we face in practice w...</td>\n",
              "      <td>Practice groups are not permitted to work on t...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3931fbe82a</td>\n",
              "      <td>Ces petites choses font une grande diff√©rence ...</td>\n",
              "      <td>J'essaye d'accomplir quelque chose.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5622f0c60b</td>\n",
              "      <td>Do you know that they can't really defend them...</td>\n",
              "      <td>They cannot protect themselves because of age.</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>86aaa48b45</td>\n",
              "      <td>‡πÄ‡∏•‡πà‡∏ô‡∏ï‡∏≤‡∏°‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏î‡πâ‡∏ß‡∏¢ ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡πà‡∏ô‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏´‡∏•‡∏≤...</td>\n",
              "      <td>‡πÄ‡∏î‡πá‡∏Å ‡πÜ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï...</td>\n",
              "      <td>th</td>\n",
              "      <td>Thai</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... label\n",
              "0  5130fd2cb5  ...     0\n",
              "1  5b72532a0b  ...     2\n",
              "2  3931fbe82a  ...     0\n",
              "3  5622f0c60b  ...     0\n",
              "4  86aaa48b45  ...     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hs-04TMOyfpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "fe0328e6-5fbe-4d20-ab08-8fcba312ccf5"
      },
      "source": [
        "#offline loading of augmented datasets\n",
        "train_twice_aug = pd.read_csv('twice_translated_aug_train.csv')\n",
        "train_twice_aug.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5130fd2cb5</td>\n",
              "      <td>and these ideas were taken into consideration ...</td>\n",
              "      <td>Interim rules developed in conjunction with th...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5b72532a0b</td>\n",
              "      <td>These are the challenges we face in practice w...</td>\n",
              "      <td>Practice groups are not allowed to deal with t...</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3931fbe82a</td>\n",
              "      <td>Ces petites choses font une grande diff√©rence ...</td>\n",
              "      <td>J'essaye d'accomplir quelque chose.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5622f0c60b</td>\n",
              "      <td>Do you know that they can't really defend them...</td>\n",
              "      <td>They cannot protect themselves because of age.</td>\n",
              "      <td>en</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>86aaa48b45</td>\n",
              "      <td>‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡πà‡∏ô‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÉ‡∏ô‡∏Å‡∏≤...</td>\n",
              "      <td>‡πÄ‡∏î‡πá‡∏Å ‡πÜ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏´‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏≤‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï...</td>\n",
              "      <td>th</td>\n",
              "      <td>Thai</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... label\n",
              "0  5130fd2cb5  ...     0\n",
              "1  5b72532a0b  ...     2\n",
              "2  3931fbe82a  ...     0\n",
              "3  5622f0c60b  ...     0\n",
              "4  86aaa48b45  ...     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZr1dCPyyfpx",
        "colab_type": "text"
      },
      "source": [
        "**Wonderful! Now you could take this dataset and apply the same procedure to generate an ever more diverse set of augmentations, or you could increase the complexity of the translation by chaining together multiple languages, i.e.**\n",
        "\n",
        "> English -> French -> Russian -> .... -> English\n",
        "\n",
        "**I encourage you to play around with this yourself to see what gives you the best performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCYQZr0AK-Og",
        "colab_type": "text"
      },
      "source": [
        "# Configure TPU\n",
        "\n",
        "**Now we have to make some TPU-training specific changes. First, we must connect to and initialize our TPU:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "5baypFOGK-Oh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "a8cbb580-6cb3-417c-b17b-9339d39151ce"
      },
      "source": [
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        print(\"Could not connect to TPU\")\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except _:\n",
        "            print(\"failed to initialize TPU\")\n",
        "    else:\n",
        "        DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE != \"TPU\":\n",
        "    print(\"Using default strategy for CPU and single GPU\")\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "if DEVICE == \"GPU\":\n",
        "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "    \n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "#REPLICAS = 8\n",
        "print(f'REPLICAS: {REPLICAS}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "connecting to TPU...\n",
            "Running on TPU  grpc://10.113.65.34:8470\n",
            "initializing  TPU ...\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.113.65.34:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.113.65.34:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU initialized\n",
            "REPLICAS: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yI04p1euK-Oi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#choose batch size - will depend on cores of our device\n",
        "BATCH_SIZE = 16 * REPLICAS"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nXqI4AK-Oo",
        "colab_type": "text"
      },
      "source": [
        "# Getting Datasets\n",
        "\n",
        "**HuggingFace Transformers makes it unbelievable easy to use transformers. In fact, you don't even need to specify the transformer or tokenizer: its architecture can be guessed from the name or path of the pretrained model you specify in the `from_pretrained` method. To read more about AutoModels/Tokenizers, see [this](https://huggingface.co/transformers/model_doc/auto.html)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gqeX9fczK-Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get HuggingFace transformers\n",
        "!pip install --quiet transformers\n",
        "\n",
        "#import model and Tokenizer\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "\n",
        "#get paths to TensorFlow XLM-RoBERTa base and large models\n",
        "roberta_base = \"jplu/tf-xlm-roberta-base\"\n",
        "roberta_large = 'jplu/tf-xlm-roberta-large'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHexYvSZyfp5",
        "colab_type": "text"
      },
      "source": [
        "**We can start by creating a simple train/validation split here and then converting them to `tf.data.Dataset` format which is the format we need to put our datasets into before we send them to the TPUs. For more about buidling data pipelines in TensorFlow, see [here](https://www.tensorflow.org/guide/data):**\n",
        "\n",
        "**Now, we do not want any samples in our validation set used to create the augmented dataset, or we will overfit our validation data. So you can either split your data into train and validation first and then augment only on train, or you can import the offline dataset and manually remove the validation indices from it and add that to the original training data**\n",
        "\n",
        "**Note that while we only apply `translation_augmentation` to the training dataset, one can add test time augmentation (TTA) and validation time augmentation (VTA). I will add TTA in the next version of this commit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UgbwgZ9Iyfp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3f6d7b8-c36e-4737-c9fa-dcd7a4e559a4"
      },
      "source": [
        "#get train, val splits\n",
        "train, val, train_labels, val_labels = train_test_split(train, train['label'],\n",
        "                                                        test_size = 0.2,\n",
        "                                                        random_state = SEED)\n",
        "\n",
        "#check size before augmenting\n",
        "print(f'Training size before augmentation: {len(train)}')\n",
        "\n",
        "if GEN_AUG:\n",
        "    #now we generate augmented data\n",
        "    train_aug = train.copy().pipe(translation_aug_parallel)\n",
        "    \n",
        "#remove validation indexs samples from our augmented dataset\n",
        "train_aug = train_aug.drop(val.index)\n",
        "train_twice_aug = train_twice_aug.drop(val.index)\n",
        "\n",
        "#and shuffle\n",
        "train_aug = shuffle(train_aug)\n",
        "train_twice_aug = shuffle(train_twice_aug)\n",
        "\n",
        "#add only twice augmented dataset to existing training dataset\n",
        "#train = train.append(train_twice_aug)\n",
        "#train_labels = train_labels.append(train_twice_aug['label'])\n",
        "\n",
        "#add both external datasets to existing training dataset\n",
        "train = train.append([train_aug, train_twice_aug])\n",
        "train_labels = train_labels.append([train_aug['label'], train_twice_aug['label']])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training size before augmentation: 9696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aPlxjgGayfp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b18006ba-fb6d-42a9-e9bb-f3ea2818d93a"
      },
      "source": [
        "#check size after augmenting\n",
        "print(f'Training size after augmentation: {len(train)}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training size after augmentation: 29088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i470uUitK-Om",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "659a2ed0-4186-42e0-fd98-287b56cef453"
      },
      "source": [
        "#create BERT tokenzer object\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(roberta_large)\n",
        "\n",
        "#define inputs to give our tokenizer (and training labels)\n",
        "train_text = train[['premise', 'hypothesis']].values.tolist()\n",
        "val_text = val[['premise', 'hypothesis']].values.tolist()\n",
        "test_text = test[['premise', 'hypothesis']].values.tolist()\n",
        "\n",
        "#tokenize our text here\n",
        "#will functionize these later\n",
        "train_enc = TOKENIZER.batch_encode_plus(\n",
        "    train_text,\n",
        "    pad_to_max_length = True,\n",
        "    max_length = MAX_LEN\n",
        ")\n",
        "\n",
        "val_enc = TOKENIZER.batch_encode_plus(\n",
        "    val_text,\n",
        "    pad_to_max_length = True,\n",
        "    max_length = MAX_LEN\n",
        ")\n",
        "\n",
        "test_enc = TOKENIZER.batch_encode_plus(\n",
        "    test_text,\n",
        "    pad_to_max_length = True,\n",
        "    max_length = MAX_LEN\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-LCYg07JK-Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert train dataset to tf.data.Dataset format\n",
        "train_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((train_enc['input_ids'], train_labels))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "#convert validation dataset to tf.data.Dataset format\n",
        "val_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((val_enc['input_ids'], val_labels))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "#convert test dataset to tf.data.Dataset format\n",
        "test_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(test_enc['input_ids'])\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uAHlCN6K-Os",
        "colab_type": "text"
      },
      "source": [
        "# Learning Rate Schedule\n",
        "\n",
        "**It is generally a good idea to use a learning rate scheduler when transfer learning. Our pretrained model already knows quite a bit, so we want to start the learning rate at 0: if we start with a high learning rate, there is a chance we 'erase' the weights that the model already had, defeating the purpose of transfer learning. We then slowly increase the learning rate as the model adapts to the new data:** \n",
        "\n",
        "**Note that I am still figuring out the best learning rate schedule as the current one does not seem to provide much increase in score/smoother training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mruu9foJK-Os",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "077512d2-cb52-4b04-9624-47cdd7670473"
      },
      "source": [
        "#define learning rate parameters\n",
        "LR_START = 5e-7\n",
        "LR_MAX = 5e-7 * REPLICAS\n",
        "LR_MIN = 1e-7\n",
        "LR_RAMPUP_EPOCHS = 3\n",
        "LR_SUSTAIN_EPOCHS = 0\n",
        "LR_DECAY = .8\n",
        "\n",
        "#stepwise schedule\n",
        "def lrfn_step(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = LR_MAX * LR_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//2)\n",
        "    return lr\n",
        "\n",
        "\n",
        "#smoothish schedule\n",
        "def lrfn_smooth(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = (LR_MAX - LR_MIN) * LR_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
        "    return lr\n",
        "    \n",
        "lr_callback_step = tf.keras.callbacks.LearningRateScheduler(lrfn_step, verbose = True)\n",
        "lr_callback_smooth = tf.keras.callbacks.LearningRateScheduler(lrfn_smooth, verbose = True)\n",
        "\n",
        "#visualize learning rate schedule\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y1 = [lrfn_step(x) for x in rng]\n",
        "y2 = [lrfn_smooth(x) for x in rng]\n",
        "fix, ax = plt.subplots(1,2)\n",
        "ax[0].plot(rng, y1)\n",
        "ax[1].plot(rng, y2)\n",
        "plt.tight_layout()\n",
        "print(\"Learning rate schedule for step schedule: {:.3g} to {:.3g} to {:.3g}\".format(y1[0], max(y1), y1[-1]))\n",
        "print(\"Learning rate schedule for smooth schedule: {:.3g} to {:.3g} to {:.3g}\".format(y2[0], max(y2), y2[-1]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate schedule for step schedule: 5e-07 to 4e-06 to 1.64e-06\n",
            "Learning rate schedule for smooth schedule: 5e-07 to 4e-06 to 7.54e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aax9hzclK-Ou",
        "colab_type": "text"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "**We will use `mish` activation here which is a non-monotonic activation function that tens to work better than `relu`. You can read more about it [here](https://arxiv.org/abs/1908.08681#:~:text=Mish%3A%20A%20Self%20Regularized%20Non%2DMonotonic%20Neural%20Activation%20Function,-Diganta%20Misra&text=In%20this%20work%2C%20a%20novel,deep%20networks%20across%20challenging%20datasets). Sadly, it is not a predefined activation function in `tf.keras` so we need to construct it ourselves, which is easy to do**\n",
        "\n",
        "**Here is a quick graphic that shows the differences between the most popular activation functions:**\n",
        "\n",
        "![](https://raw.githubusercontent.com/krutikabapat/krutikabapat.github.io/master/assets/activation.png)\n",
        "\n",
        "*Image taken from [here](https://krutikabapat.github.io/Swish-Vs-Mish-Latest-Activation-Functions/)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lUL4mgKDyfqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.math import softplus, tanh\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from tensorflow.keras.layers import Dense, Input, Activation, Dropout\n",
        "\n",
        "# softplus - log(exp(x)+1)\n",
        "def mish(x):\n",
        "    return x*tanh(softplus(x))\n",
        "\n",
        "#create misch activation to use in tf.keras\n",
        "get_custom_objects()[\"mish\"] = Activation(mish)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pbO97U7qK-Ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#helper function to create our model\n",
        "def build_model(transformer_layer, learning_rate = LR_RATE):\n",
        "    #must use this to send to TPU cores\n",
        "    with strategy.scope():\n",
        "        #define input(s)\n",
        "        input_ids = tf.keras.Input(shape = (MAX_LEN,), dtype = tf.int32)\n",
        "        \n",
        "        #insert roberta layer\n",
        "        roberta = TFAutoModel.from_pretrained(transformer_layer)\n",
        "        roberta = roberta(input_ids)[0]\n",
        "        \n",
        "        #only need <s> token here, so we extract it now\n",
        "        out = roberta[:, 0, :]\n",
        "        \n",
        "        #add optional Dense layer with dropout\n",
        "        #out = tf.keras.layers.Dropout(.2)(out)\n",
        "        \n",
        "        #two layers with mish activation\n",
        "        out = tf.keras.layers.Dense(MAX_LEN // 2, activation='mish')(out)\n",
        "        #out = tf.keras.layers.Dense(MAX_LEN // 4, activation='mish')(out)\n",
        "        \n",
        "        #two layers with relu activation\n",
        "        #out = tf.keras.layers.Dense(32, activation = 'relu')(out)\n",
        "        #out = tf.keras.layers.Dense(16, activation = 'relu')(out)\n",
        "        \n",
        "        #add our softmax layer\n",
        "        out = tf.keras.layers.Dense(3, activation = 'softmax')(out)\n",
        "        \n",
        "        #assemble model and compile\n",
        "        model = tf.keras.Model(inputs = input_ids, outputs = out)\n",
        "        model.compile(\n",
        "                        optimizer = tf.keras.optimizers.Adam(lr = LR_RATE), \n",
        "                        loss = 'sparse_categorical_crossentropy', \n",
        "                        metrics = ['accuracy'])\n",
        "        \n",
        "    return model  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50RqnVRByfqH",
        "colab_type": "text"
      },
      "source": [
        "**Some people don't believe in saving the best model and loading it after training. For one, it takes up extra space, especially if your model has hundreds of millions of parameters. In fact, you can only save 3 copies of RoBERTa-large to Kaggle memory before you run out of space**\n",
        "\n",
        "**Secondly, your training should be stable, so setting your training to a fixed number of epochs should produce similar results as using a `val_loss` monitor. If you are worried that training your model for one or two more epochs will completely throw your weights off, then you haven't found good weights for your model (for smaller models where you can easily regularize your model to train for longer periods of time). That being said, RoBERTa is a large model that can quickly overfit a dataset in a single epoch, so we will use a `val_loss` checkpoint callback:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bwZBSmpEyfqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c658e19c-d964-4433-de85-cddf7582479f"
      },
      "source": [
        "#choose to see training\n",
        "#0 for nothing, 1 for progress bar, 2 for each epoch\n",
        "VERBOSE = 2\n",
        "\n",
        "#to clear the TPU memory\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "print('---' *20)\n",
        "print(f\"Getting model...\")\n",
        "    \n",
        "#get steps per epoch values\n",
        "STEPS_PER_EPOCH = len(train) // BATCH_SIZE\n",
        "\n",
        "#make callback to save best model from training\n",
        "save_callback = tf.keras.callbacks.ModelCheckpoint(f\"best_model.h5\", monitor = 'val_loss', verbose = 0,\n",
        "                        save_best_only = True, save_weights_only = True, mode = 'min')\n",
        "\n",
        "#build and make save callback\n",
        "RoBERTa = build_model(roberta_base)\n",
        "print(f\"Training with batch size {BATCH_SIZE}\")\n",
        "\n",
        "#and train\n",
        "history = RoBERTa.fit(train_ds, steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                      validation_data = val_ds,\n",
        "                      epochs = EPOCHS, verbose = VERBOSE,\n",
        "                      #callbacks = [save_callback],\n",
        "                      callbacks = [save_callback, lr_callback_smooth]\n",
        "                      )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.113.65.34:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.113.65.34:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.113.65.34:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.113.65.34:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------\n",
            "Getting model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.modeling_tf_utils:Some weights of the model checkpoint at jplu/tf-xlm-roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "WARNING:transformers.modeling_tf_utils:All the weights of TFRobertaModel were initialized from the model checkpoint at jplu/tf-xlm-roberta-base.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training with batch size 128\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 5e-07.\n",
            "Epoch 1/12\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0114s vs `on_train_batch_end` time: 0.1556s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0114s vs `on_train_batch_end` time: 0.1556s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_test_batch_end` time: 0.0345s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_test_batch_end` time: 0.0345s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "227/227 - 64s - loss: 1.1597 - accuracy: 0.3325 - val_loss: 1.1331 - val_accuracy: 0.3313\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 1.6666666666666667e-06.\n",
            "Epoch 2/12\n",
            "227/227 - 46s - loss: 1.1154 - accuracy: 0.3451 - val_loss: 1.1142 - val_accuracy: 0.3387\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 2.833333333333333e-06.\n",
            "Epoch 3/12\n",
            "227/227 - 46s - loss: 1.1057 - accuracy: 0.3487 - val_loss: 1.0961 - val_accuracy: 0.3453\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 4e-06.\n",
            "Epoch 4/12\n",
            "227/227 - 47s - loss: 1.0863 - accuracy: 0.3908 - val_loss: 1.0663 - val_accuracy: 0.4105\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 3.22e-06.\n",
            "Epoch 5/12\n",
            "227/227 - 46s - loss: 1.0566 - accuracy: 0.4392 - val_loss: 1.0424 - val_accuracy: 0.4600\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 2.5960000000000002e-06.\n",
            "Epoch 6/12\n",
            "227/227 - 46s - loss: 0.9951 - accuracy: 0.5098 - val_loss: 0.9628 - val_accuracy: 0.5347\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 2.0968000000000004e-06.\n",
            "Epoch 7/12\n",
            "227/227 - 46s - loss: 0.9397 - accuracy: 0.5575 - val_loss: 0.9147 - val_accuracy: 0.5734\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 1.6974400000000003e-06.\n",
            "Epoch 8/12\n",
            "227/227 - 45s - loss: 0.8997 - accuracy: 0.5866 - val_loss: 0.8876 - val_accuracy: 0.5978\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 1.3779520000000001e-06.\n",
            "Epoch 9/12\n",
            "227/227 - 45s - loss: 0.8668 - accuracy: 0.6087 - val_loss: 0.8748 - val_accuracy: 0.6093\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 1.1223616000000005e-06.\n",
            "Epoch 10/12\n",
            "227/227 - 46s - loss: 0.8424 - accuracy: 0.6236 - val_loss: 0.8656 - val_accuracy: 0.6048\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 9.178892800000002e-07.\n",
            "Epoch 11/12\n",
            "227/227 - 45s - loss: 0.8230 - accuracy: 0.6346 - val_loss: 0.8577 - val_accuracy: 0.6188\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 7.543114240000003e-07.\n",
            "Epoch 12/12\n",
            "227/227 - 46s - loss: 0.8146 - accuracy: 0.6395 - val_loss: 0.8481 - val_accuracy: 0.6209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59MzQlvyfqJ",
        "colab_type": "text"
      },
      "source": [
        "**Once the model's weights have been saved to the disk, we can easily load them into an untrained model. This means we only need to train once and then we can store the weights in another notebook with other model weights to easily compare the performance of different models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cW0uYdAcyfqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "709e72a0-e06b-43f3-d7fb-bb382b17442d"
      },
      "source": [
        "#load best model\n",
        "print('Loading best model...')\n",
        "RoBERTa.load_weights(f\"best_model.h5\")\n",
        "print('Best model loaded')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading best model...\n",
            "Best model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BX7tNfSgyfqL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fcd7b906-b026-4dde-c658-880fd1b3395c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#get CSV file again\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "\n",
        "#re encode and all that\n",
        "train_text = train[['premise', 'hypothesis']].values.tolist()\n",
        "train_enc = TOKENIZER.batch_encode_plus(\n",
        "    train_text,\n",
        "    pad_to_max_length = True,\n",
        "    max_length = MAX_LEN\n",
        ")\n",
        "\n",
        "\n",
        "#tokenize our text here\n",
        "#will functionize these later\n",
        "train_enc = TOKENIZER.batch_encode_plus(\n",
        "    train_text,\n",
        "    pad_to_max_length = True,\n",
        "    max_length = MAX_LEN\n",
        ")\n",
        "\n",
        "\n",
        "#convert train dataset to tf.data.Dataset format\n",
        "train_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(train_enc['input_ids'])\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ED2LtAAyfqM",
        "colab_type": "text"
      },
      "source": [
        "# Test Time Augmentation\n",
        "\n",
        "**Here we will experiment with test time augmentation by predicting on our 3 separate test datasets and then averaging the results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0ZzfK0wVyfqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "8f4be0a9-af2c-4473-9bb3-48635f4e921d"
      },
      "source": [
        "#get external augmented test sets\n",
        "test_aug = pd.read_csv('translation_aug_test.csv')\n",
        "test_twice_aug = pd.read_csv('twice_translated_aug_test.csv')\n",
        "\n",
        "#compare different datasets\n",
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5195, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c6d58c3f69</td>\n",
              "      <td>ÿ®⁄©ÿ≥ÿå ⁄©€åÿ≥€åÿå ÿ±ÿß€Å€åŸÑÿå €åÿ≥ÿπ€åÿß€Åÿå ⁄©€åŸÑ€åÿå ⁄©€åŸÑ€åÿå ÿßŸàÿ± ⁄©ŸàŸÑŸÖ...</td>\n",
              "      <td>⁄©€åÿ≥€å ⁄©€í ŸÑÿ¶€í ⁄©Ÿàÿ¶€å €åÿßÿØ⁄Øÿßÿ± ŸÜ€Å€å⁄∫ €ÅŸà⁄Øÿß, ⁄©ŸàŸÑŸÖ€åŸÜ €Åÿßÿ¶€å...</td>\n",
              "      <td>ur</td>\n",
              "      <td>Urdu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cefcc82292</td>\n",
              "      <td>Ÿáÿ∞ÿß ŸáŸà ŸÖÿß ÿ™ŸÖ ŸÜÿµÿ≠ŸÜÿß ÿ®Ÿá.</td>\n",
              "      <td>ÿπŸÜÿØŸÖÿß Ÿäÿ™ŸÖ ÿ•ÿÆÿ®ÿßÿ±ŸáŸÖ ÿ®ŸÖÿß Ÿäÿ¨ÿ® ÿπŸÑŸäŸáŸÖ ŸÅÿπŸÑŸá ÿå ŸÅÿ¥ŸÑÿ™ ÿßŸÑ...</td>\n",
              "      <td>ar</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e98005252c</td>\n",
              "      <td>et cela est en grande partie d√ª au fait que le...</td>\n",
              "      <td>Les m√®res se droguent.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58518c10ba</td>\n",
              "      <td>‰∏éÂüéÂ∏ÇÂèäÂÖ∂‰ªñÂÖ¨Ê∞ëÂèäÁ§æÂå∫ÁªÑÁªá‰ª£Ë°®Â∞±IMAÁöÑËâ∫ÊúØÂèëÂ±ïËøõË°åÂØπËØù&amp;amp</td>\n",
              "      <td>IMA‰∏éÂÖ∂‰ªñÁªÑÁªáÂêà‰ΩúÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÈÉΩ‰æùÈù†ÂÖ±‰∫´ËµÑÈáë„ÄÇ</td>\n",
              "      <td>zh</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c32b0d16df</td>\n",
              "      <td>–û–Ω–∞ –≤—Å–µ –µ—â–µ –±—ã–ª–∞ —Ç–∞–º.</td>\n",
              "      <td>–ú—ã –¥—É–º–∞–ª–∏, —á—Ç–æ –æ–Ω–∞ —É—à–ª–∞, –æ–¥–Ω–∞–∫–æ, –æ–Ω–∞ –æ—Å—Ç–∞–ª–∞—Å—å.</td>\n",
              "      <td>ru</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... language\n",
              "0  c6d58c3f69  ...     Urdu\n",
              "1  cefcc82292  ...   Arabic\n",
              "2  e98005252c  ...   French\n",
              "3  58518c10ba  ...  Chinese\n",
              "4  c32b0d16df  ...  Russian\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ukBVOcuKyfqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "3dbd1314-0a09-44a2-e8a6-4b343f626b0c"
      },
      "source": [
        "#view augmented\n",
        "print(test_aug.shape)\n",
        "test_aug.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5195, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c6d58c3f69</td>\n",
              "      <td>ÿ®ÿß⁄©ÿ≥ÿ≤ ⁄©ÿß ŸÜÿßŸÖ ÿ®ÿß⁄©ÿ≥ ÿå ⁄©€åÿ≥€å ÿå ÿ±ÿß⁄Ü€åŸÑ ÿå €åÿ≥ÿπ€åÿß€Å ÿå ⁄©€å...</td>\n",
              "      <td>⁄©ŸàŸÑŸÖ€åŸÜ €Åÿßÿ¶€å ÿßÿ≥⁄©ŸàŸÑ ⁄©€í ÿß€å⁄© ÿ∑ÿßŸÑÿ® ÿπŸÑŸÖ ⁄©€åÿ≥€å ⁄©Ÿà ⁄©Ÿàÿ¶€å...</td>\n",
              "      <td>ur</td>\n",
              "      <td>Urdu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cefcc82292</td>\n",
              "      <td>Ÿáÿ∞ÿß ŸÜÿµÿ≠ ŸÑŸÜÿß.</td>\n",
              "      <td>ÿπŸÜÿØŸÖÿß Ÿäÿ™ŸÖ ÿ•ÿÆÿ®ÿßÿ±ŸÜÿß ÿ®ŸÖÿß Ÿäÿ¨ÿ® ÿßŸÑŸÇŸäÿßŸÖ ÿ®Ÿá ÿå ŸÑÿß ÿ™ÿ≥ŸÖÿ≠ ...</td>\n",
              "      <td>ar</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e98005252c</td>\n",
              "      <td>Et la raison la plus probable est due au fait ...</td>\n",
              "      <td>La m√®re se drogue.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58518c10ba</td>\n",
              "      <td>‰∏éÂüéÂ∏ÇÂèäÂÖ∂‰ªñÂÖ¨Ê∞ëÂèäÁ§æÂå∫ÁªÑÁªá‰ª£Ë°®Â∞±IMAÁöÑËâ∫ÊúØÂèëÂ±ïËøõË°åÂØπËØù&amp;amp</td>\n",
              "      <td>IMA‰∏éÂÖ∂‰ªñÁªÑÁªáÂêà‰ΩúÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÈÉΩ‰æùÈù†ÂÖ±‰∫´ËµÑÈáë„ÄÇ</td>\n",
              "      <td>zh</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c32b0d16df</td>\n",
              "      <td>–û–Ω–∞ –≤—Å–µ –µ—â–µ –±—ã–ª–∞ —Ç–∞–º.</td>\n",
              "      <td>–ú—ã –¥—É–º–∞–ª–∏, —á—Ç–æ –æ–Ω–∞ —É—à–ª–∞, –Ω–æ –æ–Ω–∞ –æ—Å—Ç–∞–ª–∞—Å—å.</td>\n",
              "      <td>ru</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... language\n",
              "0  c6d58c3f69  ...     Urdu\n",
              "1  cefcc82292  ...   Arabic\n",
              "2  e98005252c  ...   French\n",
              "3  58518c10ba  ...  Chinese\n",
              "4  c32b0d16df  ...  Russian\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9tJhT5lOyfqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "e3782571-d188-42c1-d03e-0b806b2c9aaa"
      },
      "source": [
        "#view twice augmented\n",
        "print(test_twice_aug.shape)\n",
        "test_twice_aug.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5195, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>lang_abv</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c6d58c3f69</td>\n",
              "      <td>ÿ®ÿß⁄©ÿ≥ÿ≤ ⁄©ÿß ŸÜÿßŸÖ ÿ®ÿß⁄©ÿ≥ ÿå ⁄©€åÿ≥€å ÿå ÿ±ÿß⁄Ü€åŸÑ ÿå €åÿ≥ÿπ€åÿß€Å ÿå ⁄©€å...</td>\n",
              "      <td>⁄©ŸàŸÑŸÖ€åŸÜ €Åÿßÿ¶€å ÿßÿ≥⁄©ŸàŸÑ ⁄©€í ÿß€å⁄© ÿ∑ÿßŸÑÿ® ÿπŸÑŸÖ ⁄©€åÿ≥€å ⁄©Ÿà ⁄©Ÿàÿ¶€å...</td>\n",
              "      <td>ur</td>\n",
              "      <td>Urdu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cefcc82292</td>\n",
              "      <td>ÿ™ŸÖ ŸÜÿµÿ≠ Ÿáÿ∞ÿß ŸÑŸÜÿß.</td>\n",
              "      <td>ÿπŸÜÿØŸÖÿß Ÿäÿ™ŸÖ ÿ•ÿÆÿ®ÿßÿ±ŸÜÿß ÿ®ŸÖÿß Ÿäÿ¨ÿ® ÿßŸÑŸÇŸäÿßŸÖ ÿ®Ÿá ÿå ŸÑÿß ÿ™ÿ≥ŸÖÿ≠ ...</td>\n",
              "      <td>ar</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e98005252c</td>\n",
              "      <td>Et la raison possible est que la m√®re prend le...</td>\n",
              "      <td>M√®re prend de la drogue.</td>\n",
              "      <td>fr</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58518c10ba</td>\n",
              "      <td>‰∏éÂüéÂ∏ÇÂèäÂÖ∂‰ªñÂÖ¨Ê∞ëÂèäÁ§æÂå∫ÁªÑÁªá‰ª£Ë°®Â∞±IMAÁöÑËâ∫ÊúØÂèëÂ±ïËøõË°åÂØπËØù&amp;amp</td>\n",
              "      <td>IMA‰∏éÂÖ∂‰ªñÁªÑÁªáÂêà‰ΩúÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ÈÉΩ‰æùÈù†ÂÖ±‰∫´ËµÑÈáë„ÄÇ</td>\n",
              "      <td>zh</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c32b0d16df</td>\n",
              "      <td>–û–Ω–∞ –≤—Å–µ –µ—â–µ –±—ã–ª–∞ —Ç–∞–º.</td>\n",
              "      <td>–ú—ã –¥—É–º–∞–ª–∏, –≤—ã —É–µ–∑–∂–∞–µ—Ç–µ, –Ω–æ –æ–Ω –æ—Å—Ç–∞–ª—Å—è.</td>\n",
              "      <td>ru</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... language\n",
              "0  c6d58c3f69  ...     Urdu\n",
              "1  cefcc82292  ...   Arabic\n",
              "2  e98005252c  ...   French\n",
              "3  58518c10ba  ...  Chinese\n",
              "4  c32b0d16df  ...  Russian\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ku9Ld8A5yfqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9acf62b1-32bd-4fb9-b548-76882ff1954c"
      },
      "source": [
        "#select relevant columns\n",
        "test_aug_text = test[['premise', 'hypothesis']].values.tolist()\n",
        "test_twice_aug_text = test[['premise', 'hypothesis']].values.tolist()\n",
        "\n",
        "#encode \n",
        "test_aug_enc = TOKENIZER.batch_encode_plus(\n",
        "    test_aug_text,\n",
        "    pad_to_max_length = True,\n",
        "    max_length = MAX_LEN\n",
        ")\n",
        "\n",
        "test_twice_aug_enc = TOKENIZER.batch_encode_plus(\n",
        "    test_twice_aug_text,\n",
        "    pad_to_max_length = True,\n",
        "    max_length = MAX_LEN\n",
        ")\n",
        "\n",
        "\n",
        "#convert to tf.data.Dataset format\n",
        "test_aug_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(test_aug_enc['input_ids'])\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n",
        "\n",
        "test_twice_aug_ds = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(test_twice_aug_enc['input_ids'])\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nQKIy-AmyfqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#finally, get all predictions\n",
        "preds =  RoBERTa.predict(test_ds)\n",
        "preds_aug = RoBERTa.predict(test_aug_ds)\n",
        "preds_twice_aug = RoBERTa.predict(test_twice_aug_ds)\n",
        "\n",
        "#and blend predictions equally\n",
        "blended_preds = (preds + preds_aug + preds_twice_aug)/3"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xcKewY66K-O1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "cab9b0f0-7d9c-412d-ec09-bfd67d6ce77d"
      },
      "source": [
        "#create submission dataframe and save\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = test['id']\n",
        "submission['preds'] = preds.argmax(axis = 1)\n",
        "submission['preds aug'] = preds_aug.argmax(axis = 1)\n",
        "submission['preds twice aug'] = preds_twice_aug.argmax(axis = 1)\n",
        "\n",
        "#choose blended predictions as final predictions\n",
        "submission['prediction'] = blended_preds.argmax(axis = 1)\n",
        "submission.head(10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>preds</th>\n",
              "      <th>preds aug</th>\n",
              "      <th>preds twice aug</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c6d58c3f69</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cefcc82292</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>e98005252c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>58518c10ba</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c32b0d16df</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>aa2510d454</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>865d1c7b16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>a16f7ed56b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6d9fa191e6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>c156e8fed5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  preds  preds aug  preds twice aug  prediction\n",
              "0  c6d58c3f69      2          2                2           2\n",
              "1  cefcc82292      2          2                2           2\n",
              "2  e98005252c      0          0                0           0\n",
              "3  58518c10ba      1          1                1           1\n",
              "4  c32b0d16df      0          0                0           0\n",
              "5  aa2510d454      0          0                0           0\n",
              "6  865d1c7b16      1          1                1           1\n",
              "7  a16f7ed56b      0          0                0           0\n",
              "8  6d9fa191e6      1          1                1           1\n",
              "9  c156e8fed5      2          2                2           2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UMZQx53sK-O3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be2b0d8f-ef1c-4af6-9f36-9ae93526d219"
      },
      "source": [
        "#save submission to disk\n",
        "submission_tta = submission[['id', 'prediction']]\n",
        "\n",
        "#be careful, files submitted must be named 'submission.csv' to submit to comp\n",
        "#and label column must be named 'prediction'\n",
        "submission_tta.to_csv('submission.csv', index =  False)\n",
        "print('Submission saved')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submission saved\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}