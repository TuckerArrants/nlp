{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Text Generation with HuggingFace - GPT2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TuckerArrants/nlp/blob/master/Text_Generation_with_HuggingFace_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKTb5QZfutrl",
        "colab_type": "text"
      },
      "source": [
        "# Experimenting with HuggingFace - Text Generation\n",
        "\n",
        "*Author: Tucker Arrants*\n",
        "\n",
        "**I have recently decided to explore the ins and outs of the ðŸ˜Š Transformers library and this is the next chapter in that journey. In this notebook, I will explore text generation using a GPT-2, which was trained the predict next words on 40GB of Internet text data. The fully trained model is actually not available as the creators were concerned about '[malicious applications of the technology](https://openai.com/blog/better-language-models/)', but there is a much smaller version that is available for enthusiants to play with, which we will use here**\n",
        "\n",
        "**In this notebook, we will explore different decoding methods like Beam search, Top-K sampling, and Top-P sampling, demonstrating their performance along the way. This project is a work in progress and I will continually update it as I learn more about text generation. Feel free to comment with any questions/suggestions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LeALJfxLutrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for reproducability\n",
        "SEED = 34\n",
        "\n",
        "#maximum number of words in output text\n",
        "MAX_LEN = 50"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QBRYwTLu931",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "a30b59e3-7120-4633-b965-4da880948d51"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCWJcyy8utrp",
        "colab_type": "text"
      },
      "source": [
        "# Intro\n",
        "\n",
        "**A language model is a machine learning model that can look at part of a sentence and predict the next word/sequence of words. Much like the autofill features on your iPhone/Android, GPT-2 is capable of next word prediction on a much larger and more sophisticated scale. For reference, the smallest available GPT-2 has 117 million parameters, whereas the largest one (invisible to the public) has over 1.5 billion parameters. The one we are playing with in this sandbox notebook has 774 million parameters**\n",
        "\n",
        "**ðŸ˜Š Transformers makes it very easy to import this model with both PyTorch and TensorFlow - in this notebook we will be using TensorFlow but it is just as easy in PyTorch. Both the model and its Tokenizer can be imported from the `transformers` library that anyone can get by typing `!pip install transformers`. Let's see just how simple it is to generate text with a neural network. We begin with our input sequence:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RySBU50Dutrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_sequence = 'I never really thought that one day I would be an adult.'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhGB1Bpautrs",
        "colab_type": "text"
      },
      "source": [
        "**We will choose the largest available GPT-2 model but it is easy to install the other sizes:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zY_1ejifutrt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "03d064c7-5b26-44b3-8dfc-9d3e6dad7d78"
      },
      "source": [
        "#get transformers\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "#get large GPT2 tokenizer and GPT2 model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#view model parameters\n",
        "GPT2.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tfgp_t2lm_head_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "transformer (TFGPT2MainLayer multiple                  774030080 \n",
            "=================================================================\n",
            "Total params: 774,030,080\n",
            "Trainable params: 774,030,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmiZUbuXutrv",
        "colab_type": "text"
      },
      "source": [
        "# First Pass (Greedy Search)\n",
        "\n",
        "**With Greedy search, the word with the highest probability is predicted as the next word i.e. the next word is updated via:**\n",
        "\n",
        "$$w_t = argmax_{w}P(w | w_{1:t-1})$$\n",
        "\n",
        "**at each timestep $t$. Let's see how this naive approach performs:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FhsMGw7Nutrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get deep learning basics\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q6kLEnnGutry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dccf9e83-b834-4a93-a9bc-05a29df69d3a"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I never really thought that one day I would be an adult. I never thought that I would be able to have a family. I never thought that I would be able to have a career. I never thought that I would be able to have a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5QTMO9sutrz",
        "colab_type": "text"
      },
      "source": [
        "**And there we go: generating text is that easy. Our results are not great - as we can see, our model starts repeating itself rather quickly. The main issue with Greedy Search is that words with high probabilities can be masked by words in front of them with low probabilities, so the model is unable to explore more diverse combinations of words. We can prevent this by implementing Beam Search:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owFZDeYzutr0",
        "colab_type": "text"
      },
      "source": [
        "# Beam Search with N-Gram Penalities\n",
        "\n",
        "**Beam search is essentially Greedy Search but the model tracks and keeps `num_beams` of hypotheses at each time step, so the model is able to compare alternative paths as it generates text. We can also include a n-gram penalty by setting `no_repeat_ngram_size = 2` which ensures that no 2-grams appear twice. We will also set `num_return_sequences = 5` so we can see what the other 5 beams looked like**\n",
        "\n",
        "**To use Beam Search, we need only modify some parameters in the `generate` function:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aAxM_v0Qutr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "14d69268-1f97-4021-c16f-3b07e624638b"
      },
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = GPT2.generate(\n",
        "    input_ids, \n",
        "    max_length = MAX_LEN, \n",
        "    num_beams = 5, \n",
        "    no_repeat_ngram_size = 2, \n",
        "    num_return_sequences = 5, \n",
        "    early_stopping = True\n",
        ")\n",
        "\n",
        "print('')\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "# now we have 3 output sequences\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I never really thought that one day I would be an adult. I thought I was going to be a kid forever.\"\n",
            "\n",
            "\"I was always a little bit of a tomboy,\" she says. \"I always wanted to play dress-up\n",
            "1: I never really thought that one day I would be an adult. I thought I was going to be a kid forever.\"\n",
            "\n",
            "\"I was always a little bit of a tomboy,\" she says. \"I didn't want to grow up and\n",
            "2: I never really thought that one day I would be an adult. I thought I was going to be a kid forever.\"\n",
            "\n",
            "\"I was always a little bit of a tomboy,\" she says. \"I didn't want to grow up.\n",
            "3: I never really thought that one day I would be an adult. I thought I was going to be a kid forever.\"\n",
            "\n",
            "\"I was always a little bit of a tomboy,\" she says. \"I didn't want to grow up to\n",
            "4: I never really thought that one day I would be an adult. I thought I was going to be a kid forever.\"\n",
            "\n",
            "\"I was always a little bit of a tomboy,\" she says. \"I always wanted to play with dolls and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DihQnXRNutr2",
        "colab_type": "text"
      },
      "source": [
        "**Now that's much better! The 5 different beam hypotheses are pretty much all the same, but if we increaed `num_beams`, then we would see some more variation in the separate beams. But of course, Beam Search is not perfect either. It works well when the legnth of the generated text is more or less constant, like problems in translation or summarization, but not so much for open-ended problems like dialog or story generation (because it is much harder to find a balance between `num_beams` and `no_repeat_ngram_size`)**\n",
        "\n",
        "**Furthermore, [research](https://arxiv.org/abs/1904.09751) shows that human languages do not follow this 'high probability word next' distribution. This makes sense - if my words were exactly what you expected them to be, I would be quite a boring person and most people don't want to be boring! The below graph plots the difference of Beam Search and actual human speech: ![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)**\n",
        "\n",
        "Taken from original paper [here](https://arxiv.org/abs/1904.09751)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__89EfTsutr3",
        "colab_type": "text"
      },
      "source": [
        "# Basic Sampling\n",
        "\n",
        "**Now we will explore indeterministic decodings - sampling. Instead of following a strict path to find the end text with the highest probability, we instead randomly pick the next word by its conditional probability distribution:**\n",
        "\n",
        "$$w_t \\sim P(w|w_{1:t-1})$$\n",
        "\n",
        "**However, when we include this randomness, the generated text tends to be incoherent (see more [here](https://arxiv.org/pdf/1904.09751.pdf)) so we can include the `temperature` parameter which increases the chances of high probability words and decreases the chances of low probability words in the sampling:**\n",
        "\n",
        "**We just need to set `do_sample = True` to implement sampling and for demonstration purposes (you'll shortly see why) we set `top_k = 0`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zC20l5uautr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5eb2c36a-9ecd-4624-97db-732b00822be4"
      },
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids, \n",
        "                             do_sample = True, \n",
        "                             max_length = MAX_LEN, \n",
        "                             top_k = 0, \n",
        "                             temperature = 0.8\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I never really thought that one day I would be an adult. I mean, I was born in 1966, at the height of the Vietnam War, so it was a very, very long way from the cold war. But I was still there at\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu6BgvzFutr5",
        "colab_type": "text"
      },
      "source": [
        "# Top-K Sampling\n",
        "\n",
        "**In Top-K sampling, the top k most likely next words are selected and the entire probability mass is shifted to these k words. So instead of increasing the chances of high probability words occuring and decreasing the chances of low probabillity words, we just remove low probability words all together**\n",
        "\n",
        "**We just need to set `top_k` to however many of the top words we want to consider for our conditional probability distribution:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d8EX3f1wutr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0eba79e6-6aff-4090-f219-0921da0c5193"
      },
      "source": [
        "#sample from only top_k most likely words\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids, \n",
        "                             do_sample = True, \n",
        "                             max_length = MAX_LEN, \n",
        "                             top_k = 50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I never really thought that one day I would be an adult. I thought life would be fair and I was just waiting for the right time. I just never really thought that I would have to worry about kids. In fact I'm sure one day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOdb2N3Eutr7",
        "colab_type": "text"
      },
      "source": [
        "**Top-K Sampling seems to generate more coherent text than our random sampling before. But we can do even better:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYMT-yHutr7",
        "colab_type": "text"
      },
      "source": [
        "# Top-P Sampling\n",
        "\n",
        "**Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely wordsm we choose the smallest set of words whose total probability is larger than p, and then the entire probability mass is shifted to the words in this set**\n",
        "\n",
        "**The main difference here is that with Top-K sampling, the size of the set of words is static (obviously) whereas in Top-P sampling, the size of the set can change. To use this sampling method, we just set `top_k = 0` and choose a value `top_p`:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xB4kIfA7utr7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9db75331-e9f4-4b70-b762-3868ddafe08c"
      },
      "source": [
        "#sample only from 80% most likely words\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids, \n",
        "                             do_sample = True, \n",
        "                             max_length = MAX_LEN, \n",
        "                             top_p = 0.8, \n",
        "                             top_k = 0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I never really thought that one day I would be an adult. So for the longest time, I've had a kind of childlike fascination with birth and childbirth, and I think that's sort of how I've been living with it.\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwQaUpOcutr9",
        "colab_type": "text"
      },
      "source": [
        "# Top-K and Top-P Sampling\n",
        "\n",
        "**As you could have probably guessed, we can use both Top-K and Top-P sampling here. This reduces the chances of us getting weird words (low probability words) while allowing for a dynamic selection size. We need only top a value for both `top_k` and `top_p`. We can even include the inital temperature parameter if we want to, Let's now see how our model performs now after adding everything together. We will check the top 5 return to see how diverse our answers are:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-Ki-f2tFutr-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "561730c9-8c83-419b-cc4e-a7812f58d014"
      },
      "source": [
        "#combine both sampling techniques\n",
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True, \n",
        "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              temperature = .9,\n",
        "                              top_k = 50, \n",
        "                              top_p = 0.8, \n",
        "                              num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I never really thought that one day I would be an adult. But now, I'm an adult. I am a grown man.\"\n",
            "\n",
            "He also said he is \"not the type of person that would get involved in that kind of stuff.\"\n",
            "\n",
            "On the eve of the anniversary of the shooting, police say they are looking for an 18-year-old man with a black hat who may have been driving a stolen truck.\n",
            "\n",
            "Police have released a photo of the suspect and ask\n",
            "\n",
            "1: I never really thought that one day I would be an adult. I just thought I'd be an adult. I think I was always a child in that sense. I don't know if I can even explain it.\"\n",
            "\n",
            "He says he's still living in a bubble, though.\n",
            "\n",
            "\"I'm not really thinking about anything outside of what's going on with me,\" he said. \"I'm in the moment, just doing my thing. I don't think about anything else.\"\n",
            "\n",
            "2: I never really thought that one day I would be an adult. I was just a kid. That was my plan.\"\n",
            "\n",
            "\"I've got to be honest with you,\" I said. \"You didn't have any intention of growing up. You were a kid.\"\n",
            "\n",
            "\"I never meant to grow up,\" she said. \"I never wanted to. I didn't know what I wanted. I wanted to be a good kid. But I just couldn't.\"\n",
            "\n",
            "\"That\n",
            "\n",
            "3: I never really thought that one day I would be an adult. I always thought that I was just going to be a child.\"\n",
            "\n",
            "He was born on Dec. 3, 1983, at the University of Chicago Medical Center in the United States, to a family of Irish immigrants.\n",
            "\n",
            "His parents, both from rural counties in Ireland, worked in factories, and he was brought up by his mother and stepfather, whom he described as \"very nice people\" who were very protective of him\n",
            "\n",
            "4: I never really thought that one day I would be an adult. But it did happen. I became a mom, and a husband, and a father. And I just felt so lucky.\"\n",
            "\n",
            "Her husband is now a father, and they are hoping to adopt a child.\n",
            "\n",
            "\"He's just the best,\" she says. \"It's just so amazing to have him.\"\n",
            "\n",
            "On the outside, she is still the same person. But she is also in a different body.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoBE3q3NutsA",
        "colab_type": "text"
      },
      "source": [
        "**Fork this notebook and experiment with your own inputs and model parameters to see what type of computer generated stories you can create**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNNIvEDDutsB",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "**Below are the only references you need to implement your own GPT-2 model for text generation**\n",
        "\n",
        "\n",
        "> https://arxiv.org/abs/1904.09751  \n",
        "> https://openai.com/blog/better-language-models/  \n",
        "> https://huggingface.co/transformers/model_doc/gpt2.html  \n",
        "> https://huggingface.co/blog/how-to-generate  "
      ]
    }
  ]
}